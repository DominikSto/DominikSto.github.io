<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="home_dominik.html">Home</a></div>
<div class="menu-item"><a href="publications_dominik.html" class="current">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<h2>Preprints:</h2>
<ol>
<li><p>D. Stöger and Y. Zhu. Non-convex matrix sensing:  Breaking the quadratic rank barrier in the sample complexity<br />
<a href="https://www.arxiv.org/abs/2408.13276" target=&ldquo;blank&rdquo;>[preprint]</a>
</p>
</li>
<li><p>H. Chou, J. Maly, and D. Stöger. How to induce regularization in generalized linear models: A guide to reparametrizing gradient flow<br />
<a href="https://arxiv.org/abs/2308.04921" target=&ldquo;blank&rdquo;>[preprint]</a>
</p>
</li>
</ol>
<h2>Refereed journal articles and machine learning conference proceedings:</h2>
<ol>
<li><p>P. Geuchen, T. Heindl, D. Stöger, and F. Voigtlaender. Upper and lower bounds for the Lipschitz constant of random neural networks<br />
<i>to appear in Information and Inference, 2025</i> <br />
<a href="https://arxiv.org/abs/2311.01356" target=&ldquo;blank&rdquo;>[preprint]</a>
</p>
</li>
<li><p>J. Kostin, F. Krahmer, and D. Stöger. How robust is randomized blind deconvolution via nuclear norm minimization against adversarial noise?<br />
<i>Applied and Computional Harmonic Analysis (ACHA), 2025</i> <br />
<a href="https://www.sciencedirect.com/science/article/abs/pii/S1063520324001234" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/2303.10030" target=&ldquo;blank&rdquo;>[preprint]</a>
</p>
</li>
<li><p>M. Soltanolkotabi, D. Stöger, C. Xie. Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing.<br />
<i>IEEE Transactions on Information Theory, 2025 (short version accepted in COLT 2023) </i> <br />
<a href="https://ieeexplore.ieee.org/document/10843299" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://proceedings.mlr.press/v195/soltanolkotabi23a.html" target=&ldquo;blank&rdquo;>[COLT version]</a><a href="https://arxiv.org/abs/2303.14244" target=&ldquo;blank&rdquo;>[preprint]</a>
<a href="Slides/AsymmetricOverparam_Paris.pdf" target=&ldquo;blank&rdquo;>[Slides]</a>
</p>
</li>
<li><p>A. Ma, D. Stöger, and Y. Zhu. Robust recovery of low-rank matrices and low-tubal-rank tensors from noisy sketches<br />
<i>SIAM Journal on Matrix Analysis and Applications (SIMAX), 2023</i> <br />
<a href="https://epubs.siam.org/eprint/QKNMCZFT7TAU2P74DTGW/full" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/2206.00803" target=&ldquo;blank&rdquo;>[preprint]</a>
</p>
</li>
<li><p>K. Lee and D. Stöger. Randomly Initialized Alternating Least Squares: Fast Convergence for Matrix Sensing.<br />
<i>SIAM Journal on Mathematics of Data Science (SIMODS), 2023</i> <br />
<a href="https://epubs.siam.org/eprint/SC8PFZVEWMEAWN4PIQBI/full" target=&ldquo;blank&rdquo;>[Journal]</a><a href="http://arxiv.org/abs/2204.11516" target=&ldquo;blank&rdquo;>[preprint]</a>
<a href="Slides/ALS_KULMUTUseminar.pdf" target=&ldquo;blank&rdquo;>[Slides]</a>
</p>
</li>
<li><p>D. Stöger and M. Soltanolkotabi. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. <br /> 
<i>NeurIPS 2021</i> <br />
<a href="https://arxiv.org/abs/2106.15013" target=&ldquo;blank&rdquo;>[full paper]</a> <a href="https://papers.nips.cc/paper/2021/hash/c82836ed448c41094025b4a872c5341e-Abstract.html" target=&ldquo;blank&rdquo;>[NeurIPS version]</a>
<a href="Slides/SmallRandomInitOberwolfach.pdf" target=&ldquo;blank&rdquo;>[Slides]</a>
</p>
</li>
<li><p>C. Kümmerle, C. Mayrink Verdun, and D. Stöger. Iteratively Reweighted Least Squares for Basis Pursuit with Global Linear Convergence Rate.<br />
<i>NeurIPS 2021 <b>(Spotlight, top 3% of submitted papers)</b></i> <br />
<a href="https://arxiv.org/abs/2012.12250" target=&ldquo;blank&rdquo;>[full paper]</a> <a href="https://proceedings.neurips.cc/paper/2021/hash/16bda725ae44af3bb9316f416bd13b1b-Abstract.html" target=&ldquo;blank&rdquo;>[NeurIPS version]</a>  
</p>
</li>
<li><p>Y. Balaji, M. Sajedi, N. Kalibhat, M. Ding, D. Stöger, M. Soltanolkotabi, S. Feizi. Understanding Over-parameterization in Generative Adversarial Networks. <br />
<i>ICLR 2021</i> <br />
<a href="https://openreview.net/pdf?id=C3qvk5IQIJY" target=&ldquo;blank&rdquo;>[Paper]</a>
</p>
</li>
<li><p>F. Krahmer and D. Stöger. On the convex geometry of blind deconvolution and matrix completion.<br /> 
<i>Communications on Pure and Applied Mathematics, 2021</i> <br />
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21957" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/1902.11156" target=&ldquo;blank&rdquo;>[preprint]</a> 
</p>
</li>
<li><p>F. Krahmer and D. Stöger. Complex phase retrieval from subgaussian measurements. <br />
<i>Journal of Fourier Analysis and Applications, 2020</i> <br />
<a href="https://link.springer.com/article/10.1007/s00041-020-09797-9" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/1906.08385" target=&ldquo;blank&rdquo;>[preprint]</a> 
</p>
</li>
<li><p>F. Cagnetti, M. Perugini, and D. Stöger. Rigidity for perimeter inequality under spherical symmetrisation<br />
<i>Calculus of Variations and Partial Differential Equations, 2020</i> <br />
<a href="https://link.springer.com/article/10.1007/s00526-020-01786-6" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/1908.04865" target=&ldquo;blank&rdquo;>[preprint]</a> 
</p>
</li>
<li><p>J. Geppert, F. Krahmer, and D. Stöger. Sparse Power Factorization: Balancing peakiness and sample complexity.<br /> 
<i>Advances in Computational Mathematics, 2019.</i> <br />
<a href="https://link.springer.com/article/10.1007/s10444-019-09698-6" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/1804.09097" target=&ldquo;blank&rdquo;>[preprint]</a> 
</p>
</li>
<li><p>P. Jung, F. Krahmer, and D. Stöger. Blind demixing and deconvolution at near-optimal rate. <br /> 
<i>IEEE Transactions on Information Theory, 2018</i> <br /> 
<a href="https://ieeexplore.ieee.org/document/8240933/" target=&ldquo;blank&rdquo;>[Journal]</a><a href="https://arxiv.org/abs/1704.04178" target=&ldquo;blank&rdquo;>[preprint]</a> 
</p>
</li>
</ol>
<h2>Book Chapters:</h2>
<ol>
<li><p>T. Fuchs, D. Gross, P. Jung, F. Krahmer, R. Kueng, Dominik Stöger. Proof methods for robust low-rank matrix recovery<br />
<i>Compressed Sensing in Information Processing. Birkhäuser, Cham, 2022. 37-75.</i> <br />
<a href="https://link.springer.com/chapter/10.1007/978-3-031-09745-4_2" target=&ldquo;blank&rdquo;>[Book chapter]</a><a href="https://arxiv.org/abs/2106.04382" target=&ldquo;blank&rdquo;>[arXiv]</a>
</p>
</li>
</ol>
<h2>Conference proceedings:</h2>
<ol>
<li><p>C. Kümmerle and D. Stöger. "Linear Convergence of Iteratively Reweighted Least Squares for Nuclear Norm Minimization”. In: IEEE 13rd Sensor Array and Multichannel Signal Processing Workshop (SAM). IEEE. 2024.
</p>
</li>
<li><p>F. Krahmer and D. Stöger. “Blind deconvolution: Convex geometry and noise robustness”. In: 52nd Annual Asilomar Conference on Signals, Systems, and Computers. IEEE. 2018.
</p>
</li>
<li><p>D. Stöger, J. Geppert, and F. Krahmer. “Sparse power factorization with refined peakiness conditions”. In: 2018 IEEE Statistical Signal Processing Workshop (SSP). IEEE. 2018, pp. 816–820.
</p>
</li>
<li><p>J. Geppert, F. Krahmer, and D. Stöger. “Refined performance guarantees for Sparse Power Factorization”. In: 12th International Conference on Sampling Theory and Applications (SampTA). IEEE. 2017, pp. 509–513.
</p>
</li>
<li><p>D. Stöger, P. Jung, and F. Krahmer. “Blind demixing and deconvolution with noisy data at near optimal rate”. In: Wavelets and Sparsity XVII. Vol. 10394. International Society for Optics and Photonics. 2017, 103941E.
</p>
</li>
<li><p>P. Jung, F. Krahmer, and D. Stoeger. “Blind Demixing and Deconvolution with Noisy Data: Near-optimal Rate”. In: 21st International ITG Workshop on Smart Antenna. 2017.
</p>
</li>
<li><p>D. Stöger, P. Jung, and F. Krahmer. “Blind deconvolution and compressed sensing”. In: 2016 4th International Workshop on Compressed Sensing Theory and its Applications to Radar, Sonar and Remote Sensing (CoSeRa). IEEE. 2016, pp. 24–27
</p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
Page generated 2025-02-26 11:22:02 CET, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
